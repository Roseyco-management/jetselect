---
phase: 09-technical-seo-infrastructure
plan: 03
type: execute
domain: seo
---

<objective>
Generate XML sitemap from Phase 8 content architecture and create robots.txt for search engine crawl guidance.

Purpose: Enable efficient search engine crawling of all 111 planned pages, communicate page priorities and update frequencies, and control crawler access to site sections via robots.txt directives.

Output: sitemap.xml (111 URLs with priority/changefreq metadata) and robots.txt (sitemap reference, crawl directives) ready for deployment and Google Search Console submission in Phase 15.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase 8 Context (Sitemap Architecture)
@.planning/phases/08-topical-map-content-architecture/08-01-SUMMARY.md
@.planning/phases/08-topical-map-content-architecture/SITEMAP.md

# Codebase Context
@improved/index.html

**Tech stack available:** Static site, manual XML file creation (no sitemap generator needed)
**Established patterns:**
- 111-page sitemap architecture from Phase 8
- URL patterns: /kopen/, /gids/, /locaties/, /vliegtuigen/, /zakelijk/, /tools/
- Bilingual URLs: Dutch primary, English secondary (/en/...)

**Constraining decisions:**
- Phase 8: 111 pages mapped with priorities (P0-P3)
- Phase 8: Content phases assigned (10/11/12/13) for production timeline
- Static deployment: Sitemap will be static XML file at domain root

**Issues being addressed:** None (creating new crawl infrastructure)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Generate XML sitemap from Phase 8 architecture</name>
  <files>improved/sitemap.xml</files>
  <action>
    Create sitemap.xml file at `improved/sitemap.xml` by transforming Phase 8 SITEMAP.md into XML format.

    **XML Structure:**
    ```xml
    <?xml version="1.0" encoding="UTF-8"?>
    <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
            xmlns:xhtml="http://www.w3.org/1999/xhtml">
      <url>
        <loc>https://jetselect.nl/</loc>
        <lastmod>2026-02-07</lastmod>
        <changefreq>weekly</changefreq>
        <priority>1.0</priority>
        <xhtml:link rel="alternate" hreflang="nl-NL" href="https://jetselect.nl/" />
        <xhtml:link rel="alternate" hreflang="en-NL" href="https://jetselect.nl/en/" />
      </url>
      <!-- Additional 110 URLs from SITEMAP.md... -->
    </urlset>
    ```

    **Priority mapping from Phase 8:**
    - P0 (pillar pages): priority="1.0"
    - P1 (cluster pages): priority="0.8"
    - P2 (supporting pages): priority="0.6"
    - P3 (long-tail pages): priority="0.4"

    **Changefreq mapping by content type:**
    - Homepage: changefreq="weekly"
    - Pillar pages: changefreq="monthly"
    - Cluster pages: changefreq="monthly"
    - Aircraft/city pages: changefreq="quarterly"
    - Blog/supporting: changefreq="yearly"

    **Bilingual URLs:** For each Dutch URL, include its English counterpart using `<xhtml:link>` tags as shown above (mirrors hreflang strategy from Plan 09-02).

    **Current status:** Since only homepage exists now, sitemap should include:
    1. Homepage (Dutch): https://jetselect.nl/
    2. Homepage (English): https://jetselect.nl/en/ (if it exists, otherwise omit)
    3. Comment block listing all 110 future URLs from SITEMAP.md with their metadata

    Format future URLs as XML comments for easy uncommenting during Phases 10-13:
    ```xml
    <!--
    Phase 10 Pillar Pages (uncomment when created):
    <url>
      <loc>https://jetselect.nl/kopen/privet-jet</loc>
      <lastmod>YYYY-MM-DD</lastmod>
      <changefreq>monthly</changefreq>
      <priority>1.0</priority>
      <xhtml:link rel="alternate" hreflang="nl-NL" href="https://jetselect.nl/kopen/privet-jet" />
      <xhtml:link rel="alternate" hreflang="en-NL" href="https://jetselect.nl/en/buy/private-jet" />
    </url>
    -->
    ```

    **What to avoid:** Do NOT include URLs that don't exist yet as active entries (only comments for future). Do NOT exceed 50MB or 50,000 URLs (we have 111, well within limits). Do NOT use relative URLs (all must be absolute https://...). Do NOT forget xmlns:xhtml namespace declaration for hreflang links. Do NOT use lastmod dates in the future (use today's date for homepage, update when pages actually published).
  </action>
  <verify>
    1. Validate XML: Use https://www.xml-sitemaps.com/validate-xml-sitemap.html to check sitemap.xml syntax
    2. Check structure: grep -c "<url>" improved/sitemap.xml shows 1 active URL (homepage) + commented future URLs
    3. Verify hreflang: Confirm xhtml:link tags present for bilingual variants
    4. Test locally: Open sitemap.xml in browser and confirm it renders as XML (no parse errors)
  </verify>
  <done>sitemap.xml created with 1 active URL (homepage) + 110 commented future URLs from Phase 8 SITEMAP.md, includes priority/changefreq metadata, bilingual hreflang links via xhtml namespace, validates in XML sitemap testing tools</done>
</task>

<task type="auto">
  <name>Task 2: Create robots.txt with sitemap reference</name>
  <files>improved/robots.txt</files>
  <action>
    Create robots.txt file at `improved/robots.txt` with proper crawl directives:

    ```txt
    # JetSelect.nl robots.txt
    # Purpose: Guide search engine crawlers, reference sitemap

    # Allow all crawlers full access (no restrictions)
    User-agent: *
    Disallow:

    # Sitemap location
    Sitemap: https://jetselect.nl/sitemap.xml

    # Optional: Crawl-delay for aggressive crawlers
    # (Uncomment if server experiences crawler load issues)
    # User-agent: *
    # Crawl-delay: 1
    ```

    **Reasoning:**
    - No Disallow rules: Static site with no admin section, login pages, or duplicate content to hide
    - Allow all bots: Want maximum search engine coverage (Google, Bing, DuckDuckGo, etc.)
    - Sitemap reference: Helps crawlers discover all pages efficiently
    - No crawl-delay by default: Static hosting can handle normal crawler load

    **Future considerations (document in comment):**
    Add comment block for future needs:
    ```txt
    # Future: When backend/admin sections added, disallow:
    # Disallow: /admin/
    # Disallow: /api/
    # Disallow: /*.json$

    # Future: When duplicate content issues arise, use canonical tags instead of Disallow
    ```

    **What to avoid:** Do NOT block CSS/JS/images (Google needs them for rendering). Do NOT block /en/ language variant (both languages should be crawlable). Do NOT add Disallow: /*.pdf$ or other file extensions without reason (PDFs can rank well). Do NOT use Noindex directive in robots.txt (use meta tag noindex in HTML head instead - robots.txt noindex is deprecated).
  </action>
  <verify>
    1. Check file exists: ls improved/robots.txt confirms file created
    2. Validate syntax: Use https://www.google.com/webmasters/tools/robots-testing-tool (or similar) to test robots.txt
    3. Verify sitemap reference: grep "Sitemap:" improved/robots.txt shows https://jetselect.nl/sitemap.xml
    4. Confirm no blocks: grep "Disallow:" improved/robots.txt shows empty Disallow (full access)
  </verify>
  <done>robots.txt created with User-agent: * + empty Disallow (allow all), sitemap reference (https://jetselect.nl/sitemap.xml), validates in robots.txt testing tools, includes future guidance comments</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] sitemap.xml validates in XML sitemap validators
- [ ] sitemap.xml includes xhtml:link hreflang tags for bilingual support
- [ ] robots.txt references correct sitemap URL (https://jetselect.nl/sitemap.xml)
- [ ] robots.txt allows all crawlers (no unintended Disallow rules)
- [ ] Both files ready for deployment to domain root
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- sitemap.xml created with 1 active URL + 110 commented future URLs
- robots.txt created with sitemap reference and open crawl access
- Sitemap includes priority/changefreq metadata from Phase 8
- Bilingual URLs supported via xhtml:link hreflang tags
- Both files validate in Google webmaster tools
</success_criteria>

<output>
After completion, create `.planning/phases/09-technical-seo-infrastructure/09-03-SUMMARY.md`:

# Phase 9 Plan 3: Sitemaps & Crawl Control Summary

**[Substantive one-liner - what sitemap infrastructure was built, not "files created"]**

## Accomplishments

- XML sitemap architected for 111 pages with priority/changefreq metadata
- robots.txt enables full crawler access with sitemap discovery
- Bilingual hreflang links integrated via xhtml namespace

## Files Created/Modified

- `improved/sitemap.xml` - 1 active URL (homepage) + 110 commented future URLs
- `improved/robots.txt` - Open access with sitemap reference

## Decisions Made

[Document any sitemap priority decisions or robots.txt choices, or "None - followed Google sitemap guidelines"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Ready for 09-04-PLAN.md (Core Web Vitals & Performance)
</output>
